### 第16章 数字人应用
数字人（Meta Human）是指通过计算机技术将真实人类的形象和行为进行数字化再现而生成的虚拟人物，通常是对真实人类的外观和行为数据进行建模及仿真实现的。数字人可以在虚拟环境中展示逼真的人类形象和行为，实现虚拟形象与真实人类之间的互动与交流。数字人可用于直播、动画、影视、游戏、教育等领域 ，也可用于虚拟主持人、聊天机器人等应用中。相关AI技术已广泛应用于数字人开发，如人脸识别、StyleGAN、first - order - model等。本章讨论数字人应用的一个特定场景：文本生成音频，音频驱动静态图像生成视频。

### 16.1 目标
音频驱动静态图像生成视频是数字人应用的常见场景，可用于直播、教学、宣传、咨询等领域。本章使用音频驱动静态图像生成视频的SadTalker和文本生成音频的coqui - ai - TTS，开发一个数字人应用。用户输入文案，通过coqui - ai - TTS生成语音（或选择已录制好的音频），再选择一张静态照片，经SadTalker处理后生成目标视频。也可将前面章节的技术加入数字人视频制作过程，如第9章用大语言模型生成文案，及第8章介绍的Stable Diffusion生成静态照片等。

### 16.2 原理

coqui - ai - TTS是将文本转换到语音的深度学习工具库，支持1100多种语言的预训练模型，可学习风格化音频来驱动单幅图像开口说话，实现逼真的3D人脸动画效果，是开源软件库。SadTalker是音频驱动图像“说话”的工具。

把coqui - ai - TTS和SadTalker技术集成到一个程序中，加入大模型Chat应用（生成文案）和Stable Diffusion（生成静态人物图像），可使数字人视频制作流程更完整。
#### 16.2.1 功能概要

1. **文本生成音频**：使用coqui - ai - TTS加载中文女声模型zh - CN/baker/tacotron2 - DDC - GST，将输入文本转成语音.wav文件。为加快生成速度，可用GPU作为计算资源。输入文本不能太短，否则生成音频结尾会有无意义噪声。

2. **音频驱动静态图像生成视频**：SadTalker处理音频数据获取梅尔频谱等特征，按输入静态图像计算面部渲染数据，根据梅尔频谱、源图像、源语义、目标语义列表及相机姿势序列生成视频，最后合成视频和音频为目标视频文件。本章例程简化了视频生成过程，大部分参数设为默认值。 

3. **Web操作界面**：使用Gradio开发Web界面，用户输入待生成音频的文本或选择录制好的音频文件，选择含人物面部图像的文件，生成视频显示在界面上供下载。开发应用了Gradio的布局、事件、变量绑定和异常处理、音频和视频组件等。



#### 16.2.2 系统架构

本章数字人应用整合文生文、文生图、文生音频、音频驱动图像生成等技术。在浏览器中运行的WebUI负责获取用户输入及显示输出，模型计算部分在后台运行。音频来源有两个：文本经模型转化生成；直接选取已录制好的音频文件。包含人脸的图像在界面上选取，音频和视频经Gradio组件上传到后台。使用SadTalker合成视频后，用Gradio组件展现视频并提供下载路径。 

![image](https://github.com/user-attachments/assets/4be0eab6-5bf5-4c6f-88e5-ea7178c17867)

![数字人应用架构](图16 - 1位置，此处无实际图片链接，原书有图)

#### 16.2.3 运行原理

1. **文本生成音频**

    - **文本预处理**：输入文本先经清洗、分词等预处理，以便后续处理。
    - **特征提取**：预处理后的文本转换成包含音素、声调、语速等信息的特征表示。 
    - **声学模型**：coqui - ai - TTS用深度学习模型学习文本与语音的映射关系，生成语音波形。 
    - **声音合成**：对生成语音进行合成处理，调整音调等，提高自然度和流畅度。 
    - **输出结果**：生成音频文件，实现文本到语音转换。 

2. **音频特征提取**：音频驱动图像生成需预处理音频文件，获取梅尔频谱图、参考系数、帧数、眨眼比例等主要特征。主要步骤包括根据音频长度和采样率计算帧数；将音频转换为梅尔频谱图；按同步步长和帧率分割梅尔频谱图；生成随机眨眼序列模拟人类表情；将数据转换为PyTorch张量并按需调整维度；得到包含梅尔频谱图、参考系数等信息的处理后数据。 

3. **图像预处理**：根据面部图像获取源图像、语义信息、目标语义信息等，步骤有加载源图像并预处理为张量；加载处理初始语义系数和生成的语义系数；转换和重复源语义匹配批处理大小；处理3DMM（三维形变模型）系数，按表情比例调整并拼接信息；生成相机姿势序列并添加到数据中；得到包含源图像、语义列表等信息的字典。 

4. **生成视频**：根据音频和照片预处理信息生成目标视频文件，步骤为用生成器模型等处理输入数据生成动画预测视频；处理视频帧数据，转换格式并调整大小；将处理后的视频帧保存为.mp4文件并添加音频；返回生成视频文件路径。 


### 16.3 开发过程
#### 16.3.1 环境准备

1. **运行环境要求**

|分类|要求|说明|
| ---- | ---- | ---- |
|操作系统|Linux或Windows|不同操作系统依赖库安装方法有别，建议用Linux，Windows可能出现scikit - image安装失败、NumPy版本问题和Python递归层数过多等问题，解决复杂|
|GPU|内存6GB以上|TTS模型小、占内存少，SadTalker运行时占内存大，内存越大运行越快|
|PyTorch|2.1.2及以上|SadTalker版本需2.1.2及以上，否则安装失败|
|CUDA|12.0及以上|PyTorch2.1.2要求CUDA最低版本为12.0，CUDA11.7无法安装PyTorch2.1.2|

2. **Python虚拟环境安装**
```bash
# 建立Python3.10环境
conda create -n meta-human python=3.10 -y
conda activate meta-human
# 更新pip
python -m pip install --upgrade pip
```
3. **SadTalker安装**
    - **代码下载**
```bash
git clone https://github.com/OpenTalker/SadTalker.git
cd SadTalker
git checkout cd4c046
```
    - **安装基础依赖库**
```bash
# 使用Conda安装FFmpeg，注意不是用pip
# 因为FFmpeg是在操作系统上调用的，而不是在Python环境里
conda install ffmpeg -y
# 使用阿里云镜像安装，还要用到pep517参数解决setuptools的兼容性问题
pip install -r requirements.txt --use-pep517 -i https://mirrors.aliyun.com/pypi/simple/
```
    - **安装coqui - ai - TTS**
```bash
# WebUI的Gradio需要coqui-ai-TTS支持
# 如果在Windows中，则需要安装Visual Studio生成工具vs_BuildTools
# 下载
https://visualstudio.microsoft.com/zh-hans/visual-cpp-build-tools/
# 安装
# 只选“使用C++的桌面开发”一个选项
# 在Linux中，有GCC（GNU Compiler Collection，GNU编译器套件）即可
# 安装TTS
pip install TTS -i https://pypi.mirrors.ustc.edu.cn/simple --trusted-host=pypi.mirrors.ustc.edu.cn
```
    - **安装PyTorch**
```bash
# 校验PyTorch是否正常安装
python -c "import torch; print(torch.cuda.is_available())"
# 如果为False，则重新安装
# 查看推理卡驱动是否支持CUDA版本
nvidia-smi
# 根据CUDA版本查找torch安装命令
# 从https://pytorch.org/首页找CUDA对应版本的安装命令
# 如果未找到，则可以在Previous versions of Torch链接中查找
# https://pytorch.org/get-started/previous-versions/
pip uninstall torch -y
pip uninstall torchvision -y
pip uninstall torchaudio -y
# CUDA版本必须是12.0及以上
pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu121
# 校验PyTorch是否正常安装
python -c "import torch; print(torch.cuda.is_available())"
```
    - **降低Gradio版本**
```bash
# 按照requirements.txt安装的Gradio运行时会报错，需要降低版本
pip install gradio==3.50.2 -i https://pypi.mirrors.ustc.edu.cn/simple --trusted-host=pypi.mirrors.ustc.edu.cn
```
    - **下载模型**
```bash
# 1. Linux
bash scripts/download_models.sh
# 2. Windows
# 下载匹配于Windows的wget，复制到c:\windows\system32路径下
https://eternallybored.org/misc/wget/
# 运行git bash
# 在SadTalker目录下空白处，右击选择Open git Bash here
# 下载模型
scripts/download_models.sh
# 或直接从https://github.com/OpenTalker/SadTalker主页上通过网盘下载并解压
# 下载完成后，目录结构如下：
SadTalker
├── checkpoints
│   ├── mapping_00109-model.pth.tar
│   ├── mapping_00229-model.pth.tar
│   ├── SadTalker_V0.0.2_256.safetensors
│   └── SadTalker_V0.0.2_512.safetensors
└── gfpgan
    ├── weights
    │   ├── alignment_WFLW_4HG.pth
    │   ├── detection_Resnet50_Final.pth
    │   ├── GFPGANv1.4.pth
    │   └── parsing_parsenet.pth
```
    - **TTS/api.py修改**：在Linux上出现TypeError: 'ModelManager' object is not subscriptable错误时，需修改TTS/api.py ，修改方法见https://github.com/coqui - ai/TTS/issues/3429 ，指定conda目录下的TTS，或用sudo find / - name TTS搜索 ，将api.py第126行改为return ModelManager(models_file=TTS.get_models_file_path(), progress_bar=False, verbose=False).list_models() 
    - **app_sadtalker.py修改**
        - 在Linux中，修改app_sadtalker.py文件的第10行代码，避免下载TTS模型，in_webui = True 。
        - 修改app_sadtalker.py的最后一行为demo.launch(server_name='0.0.0.0') 。
    - **启动程序**
        - 安装WebUI ，代码为python app_sadtalker.py ，之后访问http://127.0.0.1:7860/ 。
        - 安装客户端，代码为python inference.py --driven_audio <audio.wav> --source_image <video.mp4 or picture.png> --enhancer gfpgan ，结果保存在“/*.mp4” 。

![image](https://github.com/user-attachments/assets/e820ef53-2609-41e7-bae6-3c9fe043544c)


![SadTalker Demo程序](图16 - 2位置，此处无实际图片链接，原书有图)

4. **coqui - ai - TTS配置**
    - **模型下载**：从https://github.com/coqui - ai/TTS/releases/download/v0.0.10/tts_models--zh - CN--baker--tacotron2 - DDC - GST.zip下载模型文件 ，Windows解压到%USERPROFILE%\AppData\Local\tts ，Linux解压到/home/user/.local/share/tts 。
    - **测试验证**：新建名为tts - test.py的Python文件，内容如下
```python
import torch
from TTS.api import TTS
device = "cuda" if torch.cuda.is_available() else "cpu"
print(TTS().list_models())
tts = TTS(model_name="tts_models/zh-CN/baker/tacotron2-DDC-GST", progress_bar=False).to(device)
tts.tts_to_file(text="你好，请问你叫什么名字？", file_path="output.wav")
```
运行命令conda activate meta - human ，python tts - test.py ，应用正常运行会得到output.wav文件。

#### 16.3.2 源代码
1. **依赖库导入**
```python
import os
import sys
import shutil
import gradio as gr
import torch
from TTS.api import TTS
from src.utils.preprocess import CropAndExtract
from src.test_audio2coeff import Audio2Coeff
from src.facerenderer.animate import AnimateFromCoeff
from src.utils.init_path import init_path
from src.facerenderer import get_facerenderer_data
from src.generate_batch import get_facerenderer_batch_data
from src.generate_batch import get_data
```
2. **默认参数定义**
```python
device = "cuda" if torch.cuda.is_available() else "cpu"
tts = None
preprocess_model = None
animate_from_coeff = None
audio_to_coeff = None
preprocess = 'crop'
size = 256
# torch内存回收
def torch_gc():
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    import gc
    gc.collect()
```
3. **模型装载**
```python
def load_tts_model():
    global tts
    print(TTS().list_models())
    tts = TTS(model_name="tts_models/zh-CN/baker/tacotron2-DDC-GST", progress_bar=False).to(device)

def load_sadtalker_model():
    global preprocess_model
    global animate_from_coeff
    global audio_to_coeff
    checkpoint_path = 'checkpoints'
    os.environ['TORCH_HOME'] = checkpoint_path
    config_path ='src/config'
    sadtalker_paths = init_path(
        checkpoint_path, config_path, size, False, preprocess)
    # 根据输入的音频和姿势风格生成表情与姿势的模型
    audio_to_coeff = Audio2Coeff(sadtalker_paths, device)
    # 从视频或图像文件中提取关键信息的模型
    preprocess_model = CropAndExtract(sadtalker_paths, device)
    # 生成视频并对其进行大小调整、音频处理的模型
    animate_from_coeff = AnimateFromCoeff(sadtalker_paths, device)
```
4. **文本转音频**
```python
def text2audio(_text):
    output = "output.wav"
    if len(_text) < 10:
        raise gr.exceptions.Error('需要至少10个汉字')
    tts.tts_to_file(text=_text, file_path=output)
    return output
```
5. **生成视频**
```python
def generate_video(source_image, driven_audio):
    batch_size = 1
    exp_scale = 1.0
    use_enhancer = False
    save_dir = './results/'
    # 准备目录
    os.makedirs(save_dir, exist_ok=True)
    input_dir = os.path.join(save_dir, 'input')
    os.makedirs(input_dir, exist_ok=True)
    # 复制图像文件到input目录
    pic_path = os.path.join(input_dir, os.path.basename(source_image))
    shutil.copy2(source_image, input_dir)
    # 复制音频文件到input目录
    if driven_audio is not None and os.path.isfile(driven_audio):
        audio_path = os.path.join(input_dir, os.path.basename(driven_audio))
        shutil.copy2(driven_audio, input_dir)
    # 人脸识别
    first_frame_dir = os.path.join(save_dir, 'first_frame_dir')
    os.makedirs(first_frame_dir, exist_ok=True)
    first_coeff_path, crop_pic_path, crop_info = \
        preprocess_model.generate(pic_path, first_frame_dir, preprocess, True, size)
    if first_coeff_path is None:
        raise gr.exceptions.Error("未检测到人脸")
    # 从音频数据中提取特征
    batch = get_data(first_coeff_path, audio_path, device,
                     ref_eyeblink_path=None, still=False,
                     idlemode=False,
                     length_of_audio=0, use_blink=True)
    coeff_path = audio_to_coeff.generate(batch, save_dir, 0, None)
    # 根据音频特征准备面部渲染的数据
    data = get_facerenderer_data(coeff_path, crop_pic_path,
                                 first_coeff_path,
                                 audio_path, batch_size, still_mode=False,
                                 preprocess=preprocess,
                                 size=size, expression_scale=exp_scale)
    # 根据特征生成目标视频
    return_path = animate_from_coeff.generate(
        data, save_dir, pic_path, if use_enhancer else None,
        crop_info, enhancer='gfpgan' if use_enhancer else None,
        preprocess=preprocess, img_size=size)
    return return_path
```
```python
video_name = data['video_name']
print(f'The generated video is named {video_name} in {save_dir}')
torch_gc()
return return_path
```
### （6）WebUI
安装应用的WebUI，代码如下。
```python
def meta_demo():
    with gr.Blocks(analytics_enabled=False) as demo_interface:
        gr.Markdown("<div align='center'><h2>数字人演示</h2></div>")
        with gr.Row().style(equal_height=False):
            with gr.Column(variant='panel'):
                with gr.Tabs(elem_id="source_audio"):
                    with gr.TabItem('音频输入'):
                        tts = gr.Button(
                            '文本生成音频', elem_id="audio_generate",
                            variant='primary'
                        )
                        with gr.Column(variant='panel'):
                            input_text = gr.Textbox(
                                label='文本生成音频 [至少输入10个字]',
                                lines=5,
                                placeholder='请输入文本'
                            )
                            driven_audio = gr.Audio(
                                label="Audio", source="upload",
                                type="filepath"
                            )
                with gr.Tabs(elem_id="source_image"):
                    with gr.TabItem('照片输入'):
                        with gr.Column(variant='panel'):
                            source_image = gr.Image(
                                label="Image", source="upload",
                                type="filepath",
                                elem_id="image").style(width=512)
                with gr.Column(variant='panel'):
                    with gr.TabItem('视频输出'):
                        with gr.Tabs(elem_id="video_area"):
                            with gr.Row():
                                submit = gr.Button(
                                    '视频生成', elem_id="video_generate",
                                    variant='primary'
                                )
                            with gr.Tabs(elem_id="dist_video"):
                                gen_video = gr.Video(
                                    label="视频",
                                    format="mp4").style(width=256)
                submit.click(
                    fn=generate_video,
                    inputs=[source_image,
                            driven_audio],
                    outputs=[gen_video]
                )
                tts.click(fn=text2audio, inputs=[
                    input_text], outputs=[driven_audio])
    return demo_interface
```
### （7）程序入口
提供应用程序的入口，如下所示。
```python
if __name__ == "__main__":
    load_tts_model()
    load_sadtalker_model()
    demo = meta_demo()
    demo.queue()
    demo.launch(server_name="0.0.0.0")
```
### 16.3.3 测试
将源程序命名为meta - human.py，在Python语言环境下执行。程序运行结果如图16 - 3所示。
```bash
conda activate meta-human
python meta-human.py
```
![image](https://github.com/user-attachments/assets/063b59da-e9b1-4736-9e6d-12898ab5c643)


![数字人应用后台](图16 - 3位置，此处无实际图片链接，原书有图)

访问http://127.0.0.1:7860/，WebUI的运行结果如图16 - 4所示。


![image](https://github.com/user-attachments/assets/1763aba9-c49a-493e-9c83-752c05a9ada8)


![数字人应用WebUI](图16 - 4位置，此处无实际图片链接，原书有图) 


